{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Document Extraction - _continued_\n",
    "\n",
    "In this notebook, we will train an Amazon Comprehend custom entity recognizer so that we can detect and extract entities from the HOA document. We will be using the [Amazon Textract Parser Library](https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python) to extract the plaintext data from the document and use data science library [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) to prepare training data. We will also be needing the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/), and [AWS boto3 python sdk](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) libraries. We will perform two types of entity recognition with Amazon Comprehend.\n",
    "\n",
    "- [Default entity recognition](#step1)\n",
    "- [Custom entity recognition](#step2)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import io\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytz import timezone\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, HTML, JSON, IFrame\n",
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from textractprettyprinter.t_pretty_print import Textract_Pretty_Print, get_string\n",
    "from trp import Document\n",
    "\n",
    "# Document\n",
    "from IPython.display import Image, display, HTML, JSON\n",
    "from PIL import Image as PImage, ImageDraw\n",
    "\n",
    "\n",
    "# variables\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "os.environ[\"BUCKET\"] = data_bucket\n",
    "os.environ[\"REGION\"] = region\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role is: {role}\\nDefault SageMaker Bucket: s3://{data_bucket}\")\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "textract = boto3.client('textract', region_name=region)\n",
    "comprehend=boto3.client('comprehend', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Default Entity Recognition with Amazon Comprehend <a id=\"step1\"></a>\n",
    "\n",
    "Amazon Comprehend can detect a pre-defined list of default entities using it's pre-trained model. Check out the [documentation](https://docs.aws.amazon.com/comprehend/latest/dg/how-entities.html) for a full list of default entitied. In this section, we will see how we can use Amazon Comprehend's default entity recognizer to get the default entities present in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentName = \"docs/hoa_statement.pdf\"\n",
    "display(IFrame(documentName, 500, 600));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract the (UTF-8) string text from the document above and use the Amazon Comprehend [DetectEntities](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectEntities.html) API to detect the default entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = call_textract(input_document=f's3://{data_bucket}/idp-mortgage/textract/hoa_statement.pdf') \n",
    "lines_1 = get_string(textract_json=response_1, output_type=[Textract_Pretty_Print.LINES])\n",
    "text_1 = lines_1.replace(\"\\n\", \" \")\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities_default = comprehend.detect_entities(LanguageCode=\"en\", Text=text_1)\n",
    "\n",
    "df_default_entities = pd.DataFrame(entities_default[\"Entities\"], columns = ['Text', 'Type', 'Score'])\n",
    "df_default_entities = df_default_entities.drop_duplicates(subset=['Text']).reset_index()\n",
    "\n",
    "df_default_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows us the default entities that Amazon Comprehend was able to detect in the document's text. However, we are interested in knowing specific entity values such as the property address (which is denoted currently by default entity LOCATION), or the HOA due amount (which is denoted currently by default entity QUANTITY). In order to be able to do that, we will need to train an Amazon Comprehend custom entity recognizer which we will do in the following section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Entity Recognition with Amazon Comprehend <a id=\"step2\"></a>\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "There are 2 different ways we can train an Amazon Comprehend  custom entity recognizer. \n",
    "\n",
    "- [Annotations](https://docs.aws.amazon.com/comprehend/latest/dg/cer-annotation.html)\n",
    "- [Entity lists](https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html)\n",
    "\n",
    "Annotation method may provide more accuracy but preparing annotation data is involved. For the purposes of this hands-on we are going to train an custom entity recognizer using [entity lists](https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html). To train using entity lists we will frst need a list of entities along with sample values in a CSV format, we will also need the document's text (one line per document) in a separate plaintext file. This means every line in the training data document is a full document.\n",
    "\n",
    "Let's take a look at our sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentName = \"docs/hoa_statement.pdf\"\n",
    "display(IFrame(documentName, 500, 600));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to extract 2 entities from this document\n",
    "\n",
    "- The property address (`PROPERTY_ADDRESS`)\n",
    "- The total HOA due amount (`HOA_DUE_AMOUNT`)\n",
    "\n",
    "Since we are going to use and Entity List with the above two entities, we need to get the sample document's content in UTF-8 encoded plain text format. This can be done by extracting the text from the document file(s) using Amazon textract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_textract(input_document=f's3://{data_bucket}/idp-mortgage/textract/hoa_statement.pdf') \n",
    "lines = get_string(textract_json=response, output_type=[Textract_Pretty_Print.LINES])\n",
    "text = lines.replace(\"\\n\", \" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom entity recognizer needs atleast 200 document samples, and 250 entity samples for each entity. For the purposes of this hands-on we have provided the entity list CSV file and the document file where each line is an entire document (one document per line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Training the custom entity recognizer\n",
    "\n",
    "Let's take a look at the entity list csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities_df = pd.read_csv('./data/entity_list.csv', dtype={'Text': object})\n",
    "entities = entities_df[\"Type\"].unique().tolist()\n",
    "print(f'Custom entities : {entities}')\n",
    "print(f'\\nTotal Custom entities: {entities_df[\"Type\"].nunique()}')\n",
    "print(\"\\n\\nTotal Sample per entity:\")\n",
    "entities_df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have two entities in the entity list CSV file - 'PROPERTY_ADDRESS' and 'HOA_DUE_AMOUNT'. We also have about 300 samples per entity. With this we are now ready to train the custom entity recognizer. Let's upload the document file and entity list csv file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./data/entity_list.csv s3://{data_bucket}/idp-mortgage/comprehend/entity_list.csv\n",
    "!aws s3 cp ./data/entity_training_corpus.txt s3://{data_bucket}/idp-mortgage/comprehend/entity_training_corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize a few variables and start the entity recognizer training. Run the two code cells below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_uri = f's3://{data_bucket}/idp-mortgage/comprehend/entity_list.csv'\n",
    "training_data_uri = f's3://{data_bucket}/idp-mortgage/comprehend/entity_training_corpus.txt'\n",
    "\n",
    "print(f'Entity List CSV File: {entities_uri}')\n",
    "print(f'Training Data File: {training_data_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom entity recognizer\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "\n",
    "entity_recognizer_name = 'mortgage-custom-ner-hoa'\n",
    "entity_recognizer_version = 'v1'\n",
    "entity_recognizer_arn = ''\n",
    "create_response = None\n",
    "EntityTypes = []\n",
    "for e in entities:\n",
    "    EntityTypes.append( {'Type':e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_response = comprehend.create_entity_recognizer(\n",
    "        InputDataConfig={\n",
    "            'DataFormat': 'COMPREHEND_CSV',\n",
    "            'EntityTypes': EntityTypes,\n",
    "            'Documents': {\n",
    "                'S3Uri': training_data_uri\n",
    "            },\n",
    "            'EntityList': {\n",
    "                'S3Uri': entities_uri\n",
    "            }\n",
    "        },\n",
    "        DataAccessRoleArn=role,\n",
    "        RecognizerName=entity_recognizer_name,\n",
    "        VersionName=entity_recognizer_version,\n",
    "        LanguageCode='en'\n",
    "    )\n",
    "    \n",
    "    entity_recognizer_arn = create_response['EntityRecognizerArn']\n",
    "    \n",
    "    print(f\"Comprehend Custom entity recognizer created with ARN: {entity_recognizer_arn}\")\n",
    "except Exception as error:\n",
    "\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training may take about 20 minutes. The status of the training can be checked using the code below. You can also view the status of the training job from the Amazon Comprehend console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "jobArn = create_response['EntityRecognizerArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    describe_custom_recognizer = comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_recognizer[\"EntityRecognizerProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document entity recognizer: {status}\")\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy Custom Entity Recognizer Endpoint\n",
    "\n",
    "Our model has now been trained and can be deployed. In the next code cell, we will deploy the trained custom entity recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create comprehend endpoint\n",
    "model_arn = entity_recognizer_arn\n",
    "ep_name = 'mortgage-hoa-ner-endpoint'\n",
    "\n",
    "try:\n",
    "    endpoint_response = comprehend.create_endpoint(\n",
    "        EndpointName=ep_name,\n",
    "        ModelArn=model_arn,\n",
    "        DesiredInferenceUnits=1,    \n",
    "        DataAccessRoleArn=role\n",
    "    )\n",
    "    ER_ENDPOINT_ARN=endpoint_response['EndpointArn']\n",
    "    print(f'Endpoint created with ARN: {ER_ENDPOINT_ARN}')\n",
    "    %store ER_ENDPOINT_ARN\n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'An endpoint with the name \"{ep_name}\" already exists.')\n",
    "        ER_ENDPOINT_ARN = f'arn:aws:comprehend:{region}:{account_id}:entity-recognizer-endpoint/{ep_name}'\n",
    "        print(f'The classifier endpoint ARN is: \"{ER_ENDPOINT_ARN}\"')\n",
    "        %store ER_ENDPOINT_ARN\n",
    "    else:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the endpoint creation may take about 20 minutes. The status of the deployment can be checked using the code below. You can also view the status of the training job from the Amazon Comprehend console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "ep_arn = endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    describe_endpoint_resp = comprehend.describe_endpoint(\n",
    "        EndpointArn=ep_arn\n",
    "    )\n",
    "    status = describe_endpoint_resp[\"EndpointProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom entity recognizer classifier: {status}\")\n",
    "    \n",
    "    if status == \"IN_SERVICE\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Detect Entities using the Endpoint\n",
    "\n",
    "We will now try to detect our two custom entities 'PROPERTY_ADDRESS' and 'HOA_DUE_AMOUNT' from our sample HOA Letter. We will define a function that will call the comprehend DetectEntities API with the text extracted from textract and the enpoint. The expected output are the detected entities, their values and their corresponding confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trp import Document\n",
    "\n",
    "def get_entities(text):\n",
    "    try:\n",
    "        #detect entities\n",
    "        entities_custom = comprehend.detect_entities(LanguageCode=\"en\", Text=text, EndpointArn=ER_ENDPOINT_ARN)  \n",
    "        df_custom = pd.DataFrame(entities_custom[\"Entities\"], columns = ['Text', 'Type', 'Score'])\n",
    "        df_custom = df_custom.drop_duplicates(subset=['Text']).reset_index()\n",
    "        return df_custom\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get_entities(text)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we saw how we can train an Amazon Comprehend custom entity recognizer to detect custom entities from documents containing dense texts. We used entity lists to train the model, and eventually deployed the model with the trained model. We then used the endpoint to detect our custom entities from the text extracted by Amazon Textract, from our sample HOA Letter document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
