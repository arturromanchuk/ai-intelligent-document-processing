{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Document Classification\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Notebook\n",
    "\n",
    "In this notebook, we are going to train an Amazon Comprehend custom classifier, and deploy it behind an endpoint. We will then use the end-point to test document classification. Let's install and import some libraries that are going to be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q amazon-textract-response-parser --upgrade --force-reinstall\n",
    "!python -m pip install -q amazon-textract-caller --upgrade --force-reinstall\n",
    "!python -m pip install -q amazon-textract-prettyprinter --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from textractprettyprinter.t_pretty_print import Textract_Pretty_Print, get_string\n",
    "from trp import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from sagemaker import get_execution_role\n",
    "from IPython.display import Image, display, HTML, JSON\n",
    "\n",
    "# variables\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "os.environ[\"BUCKET\"] = data_bucket\n",
    "os.environ[\"REGION\"] = region\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role is: {role}\\nDefault SageMaker Bucket: s3://{data_bucket}\")\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "textract = boto3.client('textract', region_name=region)\n",
    "comprehend=boto3.client('comprehend', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Amazon Comprehend Custom Classification\n",
    "\n",
    "At the beginning of our document processing stage, it may not be obvious as do which documents are present in the mortgage packet. Using Amazon Comprehend custom classifier we will first identify these documents into their respective classes. Once we know which documents are present in the packet, we can run any kind of validation such as look for missing/required documents, extract specific document in a specific way such as ID documents and so on. The figure below explains this process.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/classification.png\" alt=\"cfn1\" width=\"800px\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation\n",
    "\n",
    "In order to train an Amazon Comprehend [custom classifier](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html), we will need data in UTF-8 (plaintext) format. We are going to train the classifier model in \"[Multi class](https://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-class.html)\" mode by providing it a CSV file with labeled data. \n",
    "\n",
    "The csv file must contain data in the following format\n",
    "\n",
    "```\n",
    "label, document\n",
    "```\n",
    "\n",
    "Where `label` is the document for example `PAYSTUB`, `W2`, `1099-DIV` and so on, and the document is the plaintext data extracted from each document. Since are documents are either image files or PDF files, we will use Amazon Textract `DetectDocumentText` API to get the text out of these documents and then prepare a CSV file. \n",
    "\n",
    "For example, in the code cell below we pick the W2 document and get the plain text out of it to create a comma separated output where the first field is the document lable i.e. `w2` and the second field is the text from the document itself extracted by Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = call_textract(input_document=f'docs/W2.jpg') \n",
    "# using pretty printer to get all the lines\n",
    "lines = get_string(textract_json=response, output_type=[Textract_Pretty_Print.LINES])\n",
    "row = []\n",
    "row.append('w2')\n",
    "row.append(lines)\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell example above is a single line of CSV for W2 type of document. Since, Comprehend classification model training requires a minimum of 10 sample documents per class to be able to train the model, you will repeat this process for the remaining of W2 sample documents to generate a CSV file with 10 lines. You will then repeat the process for all the other document classes to generate the corresponding CSV data for the documents.\n",
    "\n",
    "For the purposes of this lab, we have provided a csv file ready to be used to train the model. The CSV file contains 10 or more samples for each of the document types that we want to train the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_df = pd.read_csv('data/comp_class_training_data.csv', sep=\",\", names=['label','document'])\n",
    "comprehend_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload this training data CSV file into our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3 bucket:\n",
    "!aws s3 cp data/comp_class_training_data.csv s3://{data_bucket}/idp-mortgage/comprehend/comp_class_training_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Comprehend classifier training job\n",
    "\n",
    "We will now initiate an Amazon Comprehend custom classifier training job with the training data. Note that the training job may take up to 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a document classifier\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "\n",
    "document_classifier_name = 'Mortgage-Demo-Doc-Classifier'\n",
    "document_classifier_version = 'v1'\n",
    "document_classifier_arn = ''\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    create_response = comprehend.create_document_classifier(\n",
    "        InputDataConfig={\n",
    "            'DataFormat': 'COMPREHEND_CSV',\n",
    "            'S3Uri': f's3://{data_bucket}/idp-mortgage/comprehend/comp_class_training_data.csv'\n",
    "        },\n",
    "        DataAccessRoleArn=role,\n",
    "        DocumentClassifierName=document_classifier_name,\n",
    "        VersionName=document_classifier_version,\n",
    "        LanguageCode='en',\n",
    "        Mode='MULTI_CLASS'\n",
    "    )\n",
    "    \n",
    "    document_classifier_arn = create_response['DocumentClassifierArn']\n",
    "    \n",
    "    print(f\"Comprehend Custom Classifier created with ARN: {document_classifier_arn}\")\n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'A classifier with the name \"{document_classifier_name}\" already exists.')\n",
    "        document_classifier_arn = f'arn:aws:comprehend:{region}:{account_id}:document-classifier/{document_classifier_name}/version/{document_classifier_version}'\n",
    "        print(f'The classifier ARN is: \"{document_classifier_arn}\"')\n",
    "    else:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor training status of the training job\n",
    "\n",
    "Run the code cell below to monitor the status of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "jobArn = create_response['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_custom_classifier = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy model to endpoint\n",
    "\n",
    "Once the classifier training job is complete, we can deploy the trained model to an Amazon Comprehend [real time endpoint](https://docs.aws.amazon.com/comprehend/latest/dg/manage-endpoints.html). We can then call this endpoint with documents to identify which category the document belongs to.\n",
    "\n",
    "Run the following code cell to deploy an end-point with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create comprehend endpoint\n",
    "model_arn = document_classifier_arn\n",
    "ep_name = 'mortgage-doc-endpoint'\n",
    "\n",
    "try:\n",
    "    endpoint_response = comprehend.create_endpoint(\n",
    "        EndpointName=ep_name,\n",
    "        ModelArn=model_arn,\n",
    "        DesiredInferenceUnits=1,    \n",
    "        DataAccessRoleArn=role\n",
    "    )\n",
    "    ENDPOINT_ARN=endpoint_response['EndpointArn']\n",
    "    print(f'Endpoint created with ARN: {ENDPOINT_ARN}')    \n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'An endpoint with the name \"{ep_name}\" already exists.')\n",
    "        ENDPOINT_ARN = f'arn:aws:comprehend:{region}:{account_id}:document-classifier-endpoint/{ep_name}'\n",
    "        print(f'The classifier endpoint ARN is: \"{ENDPOINT_ARN}\"')\n",
    "        %store ENDPOINT_ARN\n",
    "    else:\n",
    "        print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the status of endpoint deployment\n",
    "\n",
    "Run the code cell below to monitor the status of the endpoint deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "ep_arn = endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_endpoint_resp = comprehend.describe_endpoint(\n",
    "        EndpointArn=ep_arn\n",
    "    )\n",
    "    status = describe_endpoint_resp[\"EndpointProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"IN_SERVICE\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classifying Documents\n",
    "\n",
    "Once our model is deployed with an endpoint, it's time to test it. We will pick a random document from our `/docs` directory and call the endpoint and analyze the comprehend classifier output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must extract the text from this document using textract and then use the extracted text to call the Comprehend classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract text using Amazon Textract\n",
    "response = call_textract(input_document='./docs/1099-DIV.jpg')\n",
    "text = get_string(textract_json=response, output_type=[Textract_Pretty_Print.LINES])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify document using the extracted text\n",
    "classification_response = comprehend.classify_document(Text= text, EndpointArn=ENDPOINT_ARN)\n",
    "classification_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has correctly identified the document as a 1099-DIV document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, we saw how we can train an Amazon Comprehend custom classifier with sample documents. We then deployed the trained model with an endpoint. We then extracted text from a document that we wanted to classify and then used the plain text extracted from it to call the comprehend classifier endpoint which gave us a JSON out put with all the classes and the probability of which class being the correct class for this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
